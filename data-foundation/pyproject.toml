[project]
name = "ehs-ai-data-foundation"
version = "0.1.0"
description = "EHS AI Platform - Data Foundation Component"
requires-python = ">=3.11,<3.14"
authors = [
    {name = "EHS AI Team", email = "team@ehs-ai.com"}
]
readme = "docs/README.md"
license = {text = "MIT"}

dependencies = [
    # Core dependencies
    "accelerate==1.7.0",
    "asyncio==3.4.3",
    "boto3==1.38.36",
    "botocore==1.38.36",
    "certifi>=2024.0.0",
    
    # FastAPI and web framework
    "fastapi==0.115.12",
    "fastapi-health==0.4.0",
    "uvicorn==0.34.3",
    "gunicorn==23.0.0",
    "starlette==0.46.2",
    "sse-starlette==2.3.6",
    "starlette-session==0.4.3",
    
    # LangChain ecosystem
    "langchain==0.3.25",
    "langchain-aws==0.2.25",
    "langchain-anthropic==0.3.15",
    "langchain-fireworks==0.3.0",
    "langchain-community==0.3.25",
    "langchain-core==0.3.65",
    "langchain-experimental==0.3.4",
    "langchain-google-vertexai==2.0.25",
    "langchain-groq==0.3.2",
    "langchain-openai==0.3.23",
    "langchain-text-splitters==0.3.8",
    "langchain-huggingface==0.3.0",
    "langchain-neo4j==0.4.0",
    "langsmith==0.3.45",
    "langserve==0.3.1",
    
    # LangGraph for orchestration
    "langgraph>=0.2.0",
    
    # LlamaIndex ecosystem
    "llama-index>=0.10.0",
    "llama-index-core>=0.10.0",
    "llama-index-embeddings-openai>=0.1.0",
    "llama-index-embeddings-huggingface>=0.1.0",
    "llama-index-llms-openai>=0.1.0",
    "llama-index-llms-anthropic>=0.1.0",
    
    # LlamaParse for document parsing
    "llama-parse>=0.4.0",
    "nest-asyncio>=1.5.0",
    
    # AI/ML providers
    "fireworks-ai==0.15.12",
    "openai==1.86.0",
    "anthropic>=0.25.0",
    
    # Google Cloud
    "google-api-core==2.25.1",
    "google-auth==2.40.3",
    "google_auth_oauthlib==1.2.2",
    "google-cloud-core==2.4.3",
    "google-cloud-logging==3.12.1",
    
    # Neo4j
    "neo4j-rust-ext==5.28.1.0",
    "graphdatascience==1.15.1",
    
    # Document processing
    "unstructured[all-docs]==0.17.2",
    "unstructured-client==0.36.0",
    "unstructured-inference==1.0.5",
    "PyPDF2==3.0.1",
    "PyMuPDF==1.26.1",
    "opencv-python==4.11.0.86",
    "pypandoc==1.15",
    "pypandoc-binary==1.15",
    
    # NLP and ML
    "nltk==3.9.1",
    "sentence-transformers==4.1.0",
    "langdetect==1.0.9",
    
    # Evaluation and metrics
    "ragas==0.2.15",
    "rouge_score==0.1.2",
    
    # Utilities
    "python-dotenv==1.1.0",
    "python-magic==0.4.27",
    "json-repair==0.39.1",
    "psutil==7.0.0",
    "pydantic==2.11.7",
    "tqdm>=4.66.0",
    "urllib3>=2.0.0",
    "wrapt==1.17.2",
    "yarl==1.20.1",
    "zipp==3.23.0",
    "chardet==5.2.0",
    
    # Data sources
    "wikipedia==1.4.0",
    "youtube-transcript-api==1.1.0",
    
    # Security
    "Secweb==1.18.1",
    
    # Neo4j driver (in addition to langchain-neo4j)
    "neo4j>=5.0.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.0.0",
    "pytest-asyncio>=0.21.0",
    "pytest-cov>=4.0.0",
    "black>=24.0.0",
    "ruff>=0.1.0",
    "mypy>=1.0.0",
    "pre-commit>=3.0.0",
]

torch-cpu = [
    "torch==2.3.1+cpu",
    "torchvision==0.18.1+cpu",
    "torchaudio==2.3.1+cpu",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.uv]
index-url = "https://pypi.org/simple"
extra-index-url = ["https://download.pytorch.org/whl/cpu"]

[tool.ruff]
line-length = 120
target-version = "py311"

[tool.black]
line-length = 120
target-version = ["py311"]

[tool.mypy]
python_version = "3.11"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = false

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py", "*_test.py"]
python_functions = ["test_*"]
addopts = "-v --tb=short"

